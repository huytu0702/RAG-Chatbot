import streamlit as st
import os
from dotenv import load_dotenv
# LangChain components
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Together
from langchain_core.runnables import RunnableParallel
import torch # Import th∆∞ vi·ªán torch ƒë·ªÉ ki·ªÉm tra CUDA

# --- C·∫•u h√¨nh Streamlit Page ---
st.set_page_config(page_title="Lu·∫≠t h√¨nh s·ª± RAG Chatbot", page_icon="‚öñÔ∏è")
st.title("ü§ñ Chatbot Tra c·ª©u Ph√°p Lu·∫≠t H√¨nh S·ª±")
st.info("T√¥i l√† m·ªôt tr·ª£ l√Ω AI, c√≥ th·ªÉ cung c·∫•p th√¥ng tin d·ª±a tr√™n c√°c vƒÉn b·∫£n ph√°p lu·∫≠t h√¨nh s·ª±. Vui l√≤ng nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n b√™n d∆∞·ªõi.")

# --- Load Environment Variables ---
# Load .env file at the start
load_dotenv()
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")

# Check for API Key
if not TOGETHER_API_KEY:
    st.error("TOGETHER_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong bi·∫øn m√¥i tr∆∞·ªùng. Vui l√≤ng t·∫°o file .env ho·∫∑c thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng.")
    st.stop() # D·ª´ng ·ª©ng d·ª•ng n·∫øu kh√¥ng c√≥ API key

# --- Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn RAG v·ªõi Caching (Nested Calls) ---

@st.cache_resource
def get_embedding_model():
    """Loads the embedding model, trying CUDA first then CPU."""
    model_name = "dangvantuan/vietnamese-document-embedding"
    # Determine device
    # S·ª≠ d·ª•ng torch.cuda.is_available() ƒë·ªÉ ki·ªÉm tra ch√≠nh x√°c
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    st.write(f"ƒêang t·∫£i embedding model '{model_name}' tr√™n thi·∫øt b·ªã: {device}...")

    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'trust_remote_code': True,
                          'device': device}
        )
        st.success(f"Embedding model '{model_name}' ƒë√£ t·∫£i th√†nh c√¥ng tr√™n {device}.")
        return embedding_model
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i embedding model '{model_name}' tr√™n {device}: {e}")
        # Kh√¥ng d·ª´ng ·ªü ƒë√¢y n·∫øu fallback CPU c√≥ th·ªÉ ho·∫°t ƒë·ªông,
        # nh∆∞ng trong h√†m n√†y, ch√∫ng ta ƒë√£ ch·ªçn thi·∫øt b·ªã, n√™n l·ªói l√† l·ªói nghi√™m tr·ªçng h∆°n
        st.stop() # D·ª´ng n·∫øu t·∫£i embedding th·∫•t b·∫°i

@st.cache_resource
def get_vectorstore_retriever():
    """
    Loads the Chroma vector store and creates the retriever.
    Calls get_embedding_model() internally.
    """
    PERSIST_DIRECTORY = 'vectorstore/db_chroma'
    if not os.path.exists(PERSIST_DIRECTORY):
        st.error(f"Th∆∞ m·ª•c vectorstore kh√¥ng t·ªìn t·∫°i t·∫°i {PERSIST_DIRECTORY}.")
        st.error("Vui l√≤ng ch·∫°y script t·∫°o vectorstore (v√≠ d·ª•: db_setup.py) tr∆∞·ªõc.")
        st.stop() # D·ª´ng ·ª©ng d·ª•ng n·∫øu kh√¥ng t√¨m th·∫•y vectorstore

    st.write(f"ƒêang t·∫£i vectorstore t·ª´ {PERSIST_DIRECTORY}...")
    # *** G·ªçi h√†m ƒë∆∞·ª£c cache get_embedding_model() b√™n trong ***
    embedding_model = get_embedding_model()

    try:
        vectorstore = Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=embedding_model
        )
        st.success(f"Vectorstore ƒë√£ t·∫£i th√†nh c√¥ng t·ª´ {PERSIST_DIRECTORY}")
        # T·∫°o retriever
        retriever = vectorstore.as_retriever(
            search_type='mmr', # ho·∫∑c 'similarity'
            search_kwargs={'k': 3, 'lambda_mult': 0.7}
        )
        st.success("Retriever ƒë√£ kh·ªüi t·∫°o.")
        return retriever
    except Exception as e:
        st.error(f"L·ªói khi load vectorstore t·ª´ {PERSIST_DIRECTORY}: {e}")
        st.error("Vui l√≤ng ki·ªÉm tra l·∫°i th∆∞ m·ª•c v√† n·ªôi dung c·ªßa vectorstore.")
        st.stop() # D·ª´ng n·∫øu load vectorstore th·∫•t b·∫°i

@st.cache_resource
def get_llm(api_key):
    """Initializes the Language Model."""
    model_name = "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free" # ƒê·∫∑t t√™n model v√†o bi·∫øn
    st.write(f"ƒêang kh·ªüi t·∫°o LLM model: {model_name}...")
    try:
        llm = Together(
            model=model_name,
            temperature=0.5,
            max_tokens=1024, # TƒÉng max_tokens l√™n m·ªôt ch√∫t cho c√¢u tr·∫£ l·ªùi d√†i h∆°n
            together_api_key=api_key
        )
        st.success(f"LLM ƒë√£ kh·ªüi t·∫°o v·ªõi model: {llm.model}")
        return llm
    except Exception as e:
        st.error(f"L·ªói khi kh·ªüi t·∫°o LLM model '{model_name}': {e}")
        st.error("Vui l√≤ng ki·ªÉm tra TOGETHER_API_KEY v√† k·∫øt n·ªëi m·∫°ng.")
        st.stop() # D·ª´ng n·∫øu LLM th·∫•t b·∫°i

@st.cache_resource
def get_rag_chain(api_key):
    """
    Sets up the RAG chain.
    Calls get_vectorstore_retriever() and get_llm() internally.
    """
    st.write("ƒêang thi·∫øt l·∫≠p RAG chain...")
    # *** G·ªçi c√°c h√†m ƒë∆∞·ª£c cache kh√°c b√™n trong ***
    retriever = get_vectorstore_retriever()
    llm = get_llm(api_key) # Truy·ªÅn API key v√†o ƒë√¢y

    TEMPLATE = """B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch, chuy√™n cung c·∫•p th√¥ng tin d·ª±a tr√™n vƒÉn b·∫£n ph√°p lu·∫≠t.
S·ª≠ d·ª•ng CH·ªà th√¥ng tin trong ng·ªØ c·∫£nh (context) ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
T·ªïng h·ª£p v√† t√≥m t·∫Øt th√¥ng tin li√™n quan t·ª´ ng·ªØ c·∫£nh m·ªôt c√°ch s√∫c t√≠ch v√† tr·ª±c ti·∫øp, c√≥ tr√≠ch d·∫´n ƒëi·ªÅu lu·∫≠t.
N·∫øu ng·ªØ c·∫£nh kh√¥ng ch·ª©a ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi, h√£y n√≥i "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p."
KH√îNG s·ª≠ d·ª•ng b·∫•t k·ª≥ th√¥ng tin n√†o b√™n ngo√†i ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
B·∫Øt ƒë·∫ßu c√¢u tr·∫£ l·ªùi ngay sau nh√£n "Answer:".

Context: {context}
Question: {question}
Answer:"""
    promt_template = PromptTemplate.from_template(TEMPLATE)
    st.success("Prompt template ƒë√£ t·∫°o.")

    # X√¢y d·ª±ng Chu·ªói RAG (LangChain Expression Language)
    chain = (
        RunnableParallel( # S·ª≠ d·ª•ng RunnableParallel ƒë·ªÉ chu·∫©n b·ªã input cho prompt
            context=retriever,
            question=RunnablePassthrough()
        )
        | promt_template
        | llm
        | StrOutputParser()
    )
    st.success("RAG chain ƒë√£ x√¢y d·ª±ng.")
    return chain

# --- Load the RAG Chain (will trigger loading all dependencies via caching) ---
# Ch·ªâ c·∫ßn g·ªçi h√†m get_rag_chain() ·ªü ƒë√¢y.
# Streamlit s·∫Ω t·ª± ƒë·ªông g·ªçi c√°c h√†m ph·ª• thu·ªôc (get_vectorstore_retriever, get_llm, get_embedding_model)
# nh·ªù v√†o decorator @st.cache_resource.
with st.spinner("ƒêang t·∫£i c√°c th√†nh ph·∫ßn RAG (Embedding, Vectorstore, LLM, Chain)..."):
     # Truy·ªÅn c√°c tham s·ªë c·∫ßn thi·∫øt cho c√°c h√†m ƒë∆∞·ª£c g·ªçi b√™n trong
     rag_chain = get_rag_chain(TOGETHER_API_KEY)

# --- Giao di·ªán ng∆∞·ªùi d√πng ---
st.subheader("ƒê·∫∑t c√¢u h·ªèi c·ªßa b·∫°n:")

user_question = st.text_area("Nh·∫≠p c√¢u h·ªèi v·ªÅ ph√°p lu·∫≠t:", key="question_input", height=150) # B·∫°n c√≥ th·ªÉ ch·ªânh s·ªë 150 t√πy √Ω

if st.button("T√¨m ki·∫øm"):
    if user_question:
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            try:
                response = rag_chain.invoke(user_question)
                # Lo·∫°i b·ªè ti·ªÅn t·ªë "Answer:" n·∫øu c√≥, x·ª≠ l√Ω c·∫£ in hoa/th∆∞·ªùng
                if isinstance(response, str) and response.strip().lower().startswith("answer:"):
                     response = response.strip()[len("answer:"):].strip()
                st.subheader("C√¢u tr·∫£ l·ªùi:")
                st.write(response)
            except Exception as e:
                st.error(f"ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")
                st.exception(e) # Hi·ªÉn th·ªã traceback chi ti·∫øt trong console Streamlit (h·ªØu √≠ch khi debug)

    else:
        st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")

# T√πy ch·ªçn: Th√™m th√¥ng tin v·ªÅ ngu·ªìn
st.markdown("---")
st.markdown("""
<small>·ª®ng d·ª•ng n√†y s·ª≠ d·ª•ng Embedding Model [dangvantuan/vietnamese-document-embedding](https://huggingface.co/dangvantuan/vietnamese-document-embedding), Vectorstore [Chroma DB](https://www.trychroma.com/) v√† LLM t·ª´ [Together AI](https://together.ai/).
D·ªØ li·ªáu ƒë∆∞·ª£c l·∫•y t·ª´ c√°c vƒÉn b·∫£n ph√°p lu·∫≠t ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω.</small>
""", unsafe_allow_html=True)